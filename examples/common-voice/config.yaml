# List of input datasets containing wav-files
datasets:
  - key: common-voice-4
    # Language labels as BCP-47 tags
    # See also https://schneegans.de/lv/?tags=br%0D%0Aet%0D%0Amn%0D%0Atr&format=text
    labels:
      - br
      - et
      - mn
      - tr
    splits:
        # Root directories for the training and test set metadata
        # Note that the wav-files do not need to be in these directories, since the wav-paths are defined in the utt2path files
      - key: train
        path: ./data/train
        # Use only these metadata files, ignore the rest
        datafiles:
          - utt2path
          - utt2label
      - key: test
        path: ./data/test
        datafiles:
          - utt2path
          - utt2label

# Modifying the loaded metadata
pre_initialize:
  # Shuffle metadata before reading wav files
  shuffle_utterances: true

# Pre-pre processing, after all metadata has been loaded into a dataset instance
post_initialize:
  # Check header of each wav file and drop all non-wav files
  check_wav_headers: true
  # Try to keep 5000 signals in main memory by prefetching them to a buffer
  # This might reduce latency from file IO
  num_prefetched_signals: 5000

# Pre-processing operations on signals, applied before feature extraction
pre_process:
  filters:
    # Drop all signals that do not have sample rate equal to 16k
    equal:
      key: sample_rate
      value: 16000
  # Concatenate too short signals with itself until they are at least 2 sec
  repeat_too_short_signals:
    min_length_ms: 2000
  # Create new training set signals by naive speed modifications
  # NOTE random_resampling requires tensorflow-io-nightly
  # augment:
    # - type: random_resampling
      # split: train
      # range: [0.8, 1.0]
    # - type: random_resampling
      # split: train
      # range: [1.0, 1.2]
  # RMS based VAD, dropping every frame for which rms(frame) <= strength * mean_rms
  # This is much faster than WebRTC VAD because its done with TF ops
  rms_vad:
    # Higher values will drop more frames
    strength: 0.1
    # Frame length for VAD decisions
    vad_frame_length_ms: 10
    # Minimum continous section of VAD frames marked as non-speech before they are dropped
    min_non_speech_length_ms: 100
  # Alternative VAD:
  # Apply voice activity detection with WebRTC using maximum aggressiveness level 3.
  # Very slow because we need to use tf.numpy_function outside the TF graph to use Python object webrtcvad.Vad.
  # webrtcvad:
  #   aggressiveness: 3
  #   # WebRTC VAD requires VAD windows to be 10, 20, or 30 ms
  #   vad_frame_length_ms: 10
  #   # Do not drop non-speech segments that are shorter than 500 ms
  #   min_non_speech_length_ms: 100
  # Partition all signals into chunks of 2 second, overlapping by 0.5 seconds
  chunks:
    length_ms: 2000
    step_ms: 1500
  # Serialize all pre-processed, 2 second signal chunks into a single binary file to avoid
  # repeated file IO from reading wav files
  cache:
    directory: ./lidbox-cache
    batch_size: 100
    key: signal_chunks
    log_interval: 10000
    # consume: false

# Configuration of the feature extraction pipeline
features:
  # How many signals to process in one batch.
  # This is quite conservative,
  # if memory usage is low, increasing this might help increase CPU or GPU usage
  batch_size: 200
  # Configuration for 1024-point STFT and log-scale Mel-spectrograms (or Mel filter banks)
  type: logmelspectrogram
  spectrogram:
    frame_length_ms: 25
    frame_step_ms: 10
    fft_length: 1024
  melspectrogram:
    num_mel_bins: 40
    fmin: 20
    fmax: 7000
  # Center all frequency channels to zero mean within each 2 second signal chunk, leave variances untouched
  window_normalization:
    window_len: -1
    normalize_variance: false

# Post-processing operations on extracted features before training
post_process:
  # Show 16 first samples of 50 first batches in TensorBoard for inspection
  tensorboard:
    num_batches: 50
    batch_size: 16
  # Cache extracted features into /tmp after first epoch, this might make rest of the epochs faster
  # NOTE be careful with large datasets if you have mounted /tmp to RAM
  # If you comment out the cache setting, features are extracted from the signal_chunks cache at every epoch
  cache:
    directory: /tmp/tensorflow-features/xvector
    batch_size: 100
    key: logmelspectrogram
    consume: false

# Experiment configuration for training a model
experiment:
  # Where to store checkpoints and TensorBoard events
  cache_directory: ./lidbox-cache
  # Can be anything, will be used to distinguish training runs of the same model
  name: common-voice-4
  model:
    # See lidbox/models/xvector.py
    key: xvector
    kwargs:
      channel_dropout_rate: 0.5
  optimizer:
    cls: Adam
    kwargs:
      learning_rate: 0.001
  callbacks:
    - cls: TensorBoard
    - cls: ModelCheckpoint
      format: "epoch{epoch:06d}__val_loss{val_loss:.12f}__val_sparse_categorical_accuracy{val_sparse_categorical_accuracy:.12f}.hdf5"
      kwargs:
        monitor: val_loss
        mode: min
    - cls: EarlyStopping
      kwargs:
        monitor: val_loss
        patience: 5
        mode: min
    - cls: LearningRateDateLogger
  metrics:
    - cls: SparseCategoricalAccuracy
  loss:
    cls: SparseCategoricalCrossentropy
    kwargs:
      from_logits: true
  keras_fit_kwargs:
    epochs: 50
  input_shape: [198, 40]
  output_shape: [4]
  # Which splits to use for training, validation and testing
  data:
    train:
      split: train
      batch_size: 64
      shuffle_buffer_size: 10000
    validation:
      split: test
      batch_size: 64
    test:
      split: test
      batch_size: 64
      evaluate_metrics:
        - name: sparse_average_detection_cost
        - name: sklearn_classification_report
        - name: average_f1_score
        - name: confusion_matrix

# If lidbox.dataset.pipelines.create_dataset is insufficient, a custom script can be used
# user_script: ./script.py
