# List of input datasets containing wav-files
datasets:
  - key: common-voice
    # Language labels as BCP-47 tags
    # See also https://schneegans.de/lv/?tags=br%0D%0Aet%0D%0Amn%0D%0Atr&format=text
    labels:
      - br
      - et
      - mn
      - tr
    splits:
        # Root directories for the training and test set metadata
        # Note that the wav-files do not need to be in these directories, since the wav-paths are defined in the utt2path files
      - key: train
        path: ./data/train
        # Use only these metadata files, ignore the rest
        datafiles:
          - utt2path
          - utt2label
      - key: test
        path: ./data/test
        datafiles:
          - utt2path
          - utt2label

# After loading all metadata, drop invalid wav files and shuffle all files
post_initialize:
  check_wav_headers: true
  shuffle_buffer_size: 200000
  # Try to keep 5000 signals in main memory by prefetching them to a buffer
  # This might reduce latency from file IO
  num_prefetched_signals: 5000

# Pre-processing operations on signals, applied before feature extraction
pre_process:
  filters:
    # Drop all signals that do not have sample rate equal to 16k
    equal:
      key: sample_rate
      value: 16000
  # Concatenate too short signals with itself until they are at least 2 sec
  repeat_too_short_signals:
    min_length_ms: 2000
  # Create new training set signals by naive speed modifications
  # NOTE resampling requires tensorflow-io-nightly
  # augment:
    # - type: random_resampling
      # split: train
      # range: [0.8, 1.0]
    # - type: random_resampling
      # split: train
      # range: [1.0, 1.2]
  # Apply voice activity detection with WebRTC using aggressiveness level 0
  # Very slow because we need to use tf.numpy_function for webrtcvad.Vad
  webrtcvad:
    aggressiveness: 0
    # WebRTC VAD requires VAD windows to be 10, 20, or 30 ms
    vad_frame_length_ms: 10
    # Do not drop non-speech segments that are shorter than 500 ms
    min_non_speech_length_ms: 500
  # # Alternative VAD:
  # # RMS based VAD, dropping every frame for which rms(frame) <= strength * mean_rms
  # # This is much faster than WebRTC VAD because its done with TF ops
  # rms_vad:
  #   strength: 0.1
  #   vad_frame_length_ms: 25
  #   min_non_speech_length_ms: 500
  # Partition all signals into non-overlapping chunks of 2 second
  chunks:
    length_ms: 2000
    step_ms: 2000
  cache:
    directory: ./lidbox-cache
    batch_size: 100
    key: signal_chunks
    log_interval: 10000

# Configuration of the feature extraction pipeline
features:
  # How many signals to process in one batch.
  # This is quite conservative, increasing this might help increase CPU or GPU usage
  batch_size: 200
  # Configuration for 1024-point STFT and log-scale Mel-spectrograms (or Mel filter banks)
  type: logmelspectrogram
  spectrogram:
    frame_length_ms: 25
    frame_step_ms: 10
    fft_length: 1024
  melspectrogram:
    num_mel_bins: 64
    fmin: 20
    fmax: 7000
  # Mean-normalization with sliding window over 100 feature frames (i.e. 1 second with 10 ms frame step)
  window_normalization:
    window_len: 100
    normalize_variance: false

# Post-processing operations on extracted features before training
post_process:
  # Show 16 first samples of 50 first batches in TensorBoard for inspection
  tensorboard:
    num_batches: 50
    batch_size: 16

# Experiment configuration for training a model
experiment:
  # Where to store checkpoints and TensorBoard events
  cache_directory: ./lidbox-cache
  # Can be anything, will be used to distinguish training runs of the same model
  name: commonvoice-lang4
  model:
    # See lidbox/models/xvector.py
    key: xvector
  optimizer:
    cls: Adam
    kwargs:
      learning_rate: 0.0001
  callbacks:
    - cls: TensorBoard
    - cls: ModelCheckpoint
      format: "epoch{epoch:06d}__val_loss{val_loss:.12f}__val_sparse_categorical_accuracy{val_sparse_categorical_accuracy:.12f}.hdf5"
      kwargs:
        monitor: val_loss
        mode: min
    - cls: EarlyStopping
      kwargs:
        monitor: val_loss
        patience: 5
        mode: min
    - cls: LearningRateDateLogger
  metrics:
    - cls: SparseCategoricalAccuracy
  loss:
    cls: SparseCategoricalCrossentropy
    kwargs:
      from_logits: true
  keras_fit_kwargs:
    epochs: 50
  input_shape: [198, 64]
  output_shape: [4]
  # Which splits to use for training, validation and testing
  data:
    train:
      split: train
      batch_size: 64
      shuffle_buffer_size: 10000
    validation:
      split: test
      batch_size: 16
    test:
      split: test
      batch_size: 16
      evaluate_metrics:
        - name: sparse_average_detection_cost
        - name: sklearn_classification_report
        - name: average_f1_score
        - name: confusion_matrix

# If lidbox.dataset.pipelines.create_dataset is insufficient, a custom script can be used
# user_script: ./script.py
